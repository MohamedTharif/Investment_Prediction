import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout
from datetime import datetime , timedelta
import keras_tuner as kt



import warnings
warnings.filterwarnings("ignore")


import torch
print(torch.cuda.is_available())



# Define parameters
prediction_days = 120

# Download stock data
start_date=  datetime.now() - timedelta(days=365)
end_date=datetime.today()

stock_data = yf.download('TATASTEEL.NS', start=start_date, end=end_date)




# Prepare training data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(stock_data['Close'].values.reshape(-1, 1))

x_train = []
y_train = []

for x in range(prediction_days, len(scaled_data)):
    x_train.append(scaled_data[x - prediction_days:x, 0])
    y_train.append(scaled_data[x, 0])

x_train, y_train = np.array(x_train), np.array(y_train)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))




# Define the model building function
def build_model(hp):
    model = Sequential()
    model.add(LSTM(units=hp.Int('units_1', min_value=50, max_value=100, step=10),
                   return_sequences=True,
                   input_shape=(x_train.shape[1], 1)))
    model.add(BatchNormalization())
    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))
    
    model.add(LSTM(units=hp.Int('units_2', min_value=50, max_value=100, step=10),
                   return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))
    
    model.add(LSTM(units=hp.Int('units_3', min_value=50, max_value=100, step=10)))
    model.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)))
    
    model.add(Dense(units=25, activation='relu'))
    model.add(Dense(units=1))
    
    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),
                  loss='mean_squared_error')
     
    # Move Model to GPU
    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    #model.to(device)
    
    return model




# Set up Keras Tuner
tuner = kt.Hyperband(build_model,
                     objective='val_loss',
                     max_epochs=25,
                     hyperband_iterations=2,
                     directory='my_dir',
                     project_name='stock_price_tuning')




# Split the data for validation
split = int(0.8 * len(x_train))
x_train_split, x_val_split = x_train[:split], x_train[split:]
y_train_split, y_val_split = y_train[:split], y_train[split:]




# Search for the best hyperparameters
tuner.search(x_train_split, y_train_split,
             epochs=25,
             batch_size=32,
             validation_data=(x_val_split, y_val_split))




# Retrieve the best hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters()[0]

# Build and compile the best model
best_model = build_model(best_hyperparameters)

# Train the best model on the full training data
best_model.fit(x_train, y_train, epochs=25, batch_size=32)



# Initialize the scaler
scaler = MinMaxScaler()

# Fit scaler only on training data
scaler.fit(stock_data['Close'].values.reshape(-1, 1))

# Normalize training data
scaled_train_data = scaler.transform(stock_data['Close'].values.reshape(-1, 1))

# Prepare test data (make sure to use the same scaler)
test_data = yf.download('TATASTEEL.NS', start=start_date, end=end_date)
actual_prices = test_data['Close'].values

# Normalize test data using the same scaler
scaled_test_data = scaler.transform(test_data['Close'].values.reshape(-1, 1))

# Combine and prepare model inputs as before
total_dataset = np.concatenate((scaled_train_data, scaled_test_data), axis=0)
model_inputs = total_dataset[len(total_dataset) - len(scaled_test_data) - prediction_days:].reshape(-1, 1)

# Prepare x_test for prediction (same as before)
x_test = []
for x in range(prediction_days, len(model_inputs)):
    x_test.append(model_inputs[x - prediction_days:x, 0])

x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

# Make prediction for tomorrow
predicted_price = best_model.predict(x_test[-1].reshape(1, prediction_days, 1))




# Make predictions
predicted_prices = best_model.predict(x_test)
predicted_prices = scaler.inverse_transform(predicted_prices)

# Get the last prediction
real_data = model_inputs[-prediction_days:].reshape(1, prediction_days, 1)
prediction = best_model.predict(real_data)
prediction = scaler.inverse_transform(prediction)

# Print results
print(f"Last actual closing price: {actual_prices[-1]}")
print(f"Last predicted closing price: {prediction[0][0]}")



from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Assuming `predicted_prices` and `actual_prices` are your predictions and true values

# Calculate MAE, MSE, and RMSE
mae = mean_absolute_error(actual_prices[-len(predicted_prices):], predicted_prices)
mse = mean_squared_error(actual_prices[-len(predicted_prices):], predicted_prices)
rmse = np.sqrt(mse)

# Calculate MAPE
actual_values = actual_prices[-len(predicted_prices):]
mape = np.mean(np.abs((actual_values - predicted_prices.flatten()) / actual_values)) * 100

# Calculate accuracy percentage
accuracy_percentage = 100 - mape

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Accuracy Percentage: {accuracy_percentage:.2f}%")



# Save the entire model
best_model.save("D:\Projects\ML_Project\Jupyter NoteBookstock_prediction_mode.h5")

