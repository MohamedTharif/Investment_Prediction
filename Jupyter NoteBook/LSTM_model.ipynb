{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19184744-ceb3-4650-98ed-6454c0889fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from datetime import datetime , timedelta\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9634dde-0ed7-45a7-bd54-52e6824106a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7ca760-e138-4f4b-b568-f5f188f17cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be49936d-14f8-4dab-9eef-a2ec19bd6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define parameters\n",
    "prediction_days = 120\n",
    "\n",
    "# Download stock data\n",
    "start_date=  datetime.now() - timedelta(days=365)\n",
    "end_date=datetime.today()\n",
    "\n",
    "stock_data = yf.download('TATASTEEL.NS', start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a76cff-d05b-43d4-a5d9-85f6c6d06143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare training data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(stock_data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for x in range(prediction_days, len(scaled_data)):\n",
    "    x_train.append(scaled_data[x - prediction_days:x, 0])\n",
    "    y_train.append(scaled_data[x, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d31ae1-bdd9-445b-bb4a-2047c70b7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units_1', min_value=50, max_value=100, step=10),\n",
    "                   return_sequences=True,\n",
    "                   input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(LSTM(units=hp.Int('units_2', min_value=50, max_value=100, step=10),\n",
    "                   return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(LSTM(units=hp.Int('units_3', min_value=50, max_value=100, step=10)))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "                  loss='mean_squared_error')\n",
    "     \n",
    "    # Move Model to GPU\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model.to(device)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a481adfa-d5cb-4d0c-b410-2bb0b8a134fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\stock_price_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Keras Tuner\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=25,\n",
    "                     hyperband_iterations=2,\n",
    "                     directory='my_dir',\n",
    "                     project_name='stock_price_tuning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc5eafe-3f31-4b88-86c8-d011860276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data for validation\n",
    "split = int(0.8 * len(x_train))\n",
    "x_train_split, x_val_split = x_train[:split], x_train[split:]\n",
    "y_train_split, y_val_split = y_train[:split], y_train[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87fcc50a-188b-4106-b47b-74e903b131e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(x_train_split, y_train_split,\n",
    "             epochs=25,\n",
    "             batch_size=32,\n",
    "             validation_data=(x_val_split, y_val_split))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e8e940-a199-43fd-a78f-39059c3c315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 102ms/step - loss: 0.4795\n",
      "Epoch 2/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.2683\n",
      "Epoch 3/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1251\n",
      "Epoch 4/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.1164\n",
      "Epoch 5/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0870\n",
      "Epoch 6/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0945\n",
      "Epoch 7/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0542\n",
      "Epoch 8/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0602\n",
      "Epoch 9/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0594\n",
      "Epoch 10/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0653\n",
      "Epoch 11/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0504\n",
      "Epoch 12/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0566\n",
      "Epoch 13/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0624\n",
      "Epoch 14/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0509\n",
      "Epoch 15/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0458\n",
      "Epoch 16/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0274\n",
      "Epoch 17/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0442\n",
      "Epoch 18/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0374\n",
      "Epoch 19/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0387\n",
      "Epoch 20/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0308\n",
      "Epoch 21/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0373\n",
      "Epoch 22/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0293\n",
      "Epoch 23/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - loss: 0.0307\n",
      "Epoch 24/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0348\n",
      "Epoch 25/25\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b61f817950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Build and compile the best model\n",
    "best_model = build_model(best_hyperparameters)\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(x_train, y_train, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6759aff2-b7d0-44b8-9d3c-743128754d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler only on training data\n",
    "scaler.fit(stock_data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "# Normalize training data\n",
    "scaled_train_data = scaler.transform(stock_data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "# Prepare test data (make sure to use the same scaler)\n",
    "test_data = yf.download('TATASTEEL.NS', start=start_date, end=end_date)\n",
    "actual_prices = test_data['Close'].values\n",
    "\n",
    "# Normalize test data using the same scaler\n",
    "scaled_test_data = scaler.transform(test_data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "# Combine and prepare model inputs as before\n",
    "total_dataset = np.concatenate((scaled_train_data, scaled_test_data), axis=0)\n",
    "model_inputs = total_dataset[len(total_dataset) - len(scaled_test_data) - prediction_days:].reshape(-1, 1)\n",
    "\n",
    "# Prepare x_test for prediction (same as before)\n",
    "x_test = []\n",
    "for x in range(prediction_days, len(model_inputs)):\n",
    "    x_test.append(model_inputs[x - prediction_days:x, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Make prediction for tomorrow\n",
    "predicted_price = best_model.predict(x_test[-1].reshape(1, prediction_days, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf5cb95-aacd-4597-9505-5190b4487b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Last actual closing price: 164.36000061035156\n",
      "Last predicted closing price: 144.41943359375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions\n",
    "predicted_prices = best_model.predict(x_test)\n",
    "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
    "\n",
    "# Get the last prediction\n",
    "real_data = model_inputs[-prediction_days:].reshape(1, prediction_days, 1)\n",
    "prediction = best_model.predict(real_data)\n",
    "prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "# Print results\n",
    "print(f\"Last actual closing price: {actual_prices[-1]}\")\n",
    "print(f\"Last predicted closing price: {prediction[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09172b4-da56-44cf-8d30-274f71d358b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 13.606145577352555\n",
      "Mean Squared Error: 264.42942261080054\n",
      "Root Mean Squared Error: 16.261286007287385\n",
      "Mean Absolute Percentage Error (MAPE): 8.81%\n",
      "Accuracy Percentage: 91.19%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `predicted_prices` and `actual_prices` are your predictions and true values\n",
    "\n",
    "# Calculate MAE, MSE, and RMSE\n",
    "mae = mean_absolute_error(actual_prices[-len(predicted_prices):], predicted_prices)\n",
    "mse = mean_squared_error(actual_prices[-len(predicted_prices):], predicted_prices)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MAPE\n",
    "actual_values = actual_prices[-len(predicted_prices):]\n",
    "mape = np.mean(np.abs((actual_values - predicted_prices.flatten()) / actual_values)) * 100\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "accuracy_percentage = 100 - mape\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Accuracy Percentage: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3370490-6b69-46fd-9fcb-87f55f296bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "best_model.save(\"D:\\Projects\\Investment _Prediction\\Jupyter NoteBook\\lstm_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
